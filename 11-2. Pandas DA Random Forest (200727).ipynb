{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>■ Random Forest</b>\n",
    "    의사결정트리 + Ensemble\n",
    "    \n",
    "    Ensemble\n",
    "        약한 학습자를 여러개 결합하면 강한 학습자를 만들 수 있는 아이디어를 기반으로 하고\n",
    "        앙상블 모형은 여러 개의 분류 모형을 같이 사용하여 한꺼번에 평가하는 모델\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T05:04:56.578211Z",
     "start_time": "2020-07-27T05:04:56.323367Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7454909819639278\n",
      "[[102  24]\n",
      " [ 18  71]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.81      0.83       126\n",
      "           1       0.75      0.80      0.77        89\n",
      "\n",
      "    accuracy                           0.80       215\n",
      "   macro avg       0.80      0.80      0.80       215\n",
      "weighted avg       0.81      0.80      0.81       215\n",
      "\n",
      "0.8046511627906977\n"
     ]
    }
   ],
   "source": [
    "# 예제 : seaborn 타이타닉 데이터로 Random Forest 모델 생성\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "df = sns.load_dataset('titanic')\n",
    "\n",
    "# 컬럼이 모두다 출력될 수 있도록 출력할 열의 개수 한도를 늘리기\n",
    "pd.set_option('display.max_columns',15)\n",
    "\n",
    "# 2단계 결측치 확인하고 제거하거나 치환한다.\n",
    "# 2.1 타이타닉 데이터 프레임의 자료형을 확인한다.\n",
    "mask4 = (df.age<10) | (df.sex=='female') \n",
    "df['child_women'] = mask4.astype(int)\n",
    "\n",
    "rdf = df.drop(['deck','embark_town'], axis =1)\n",
    "rdf = rdf.dropna( subset=['age'], how='any', axis=0)\n",
    "\n",
    "most_freq = rdf['embarked'].value_counts().idxmax()\n",
    "rdf['embarked'].fillna(most_freq, inplace = True)\n",
    "\n",
    "ndf = rdf[['survived','pclass','sex','age','sibsp','parch','embarked','child_women']]\n",
    "\n",
    "# 파이썬의 의사결정트리 모델을 사용하려면 데이터가 다 숫자여야한다.\n",
    "# 랜덤포레스트 : 의사결정트리 + Bagging\n",
    "\n",
    "gender = pd.get_dummies(ndf['sex'])\n",
    "ndf = pd.concat([ndf,gender], axis= 1)\n",
    "onehot_embarked = pd.get_dummies(ndf['embarked'])\n",
    "ndf = pd.concat([ndf,onehot_embarked],axis=1)\n",
    "ndf.drop(['sex','embarked'], axis=1, inplace = True)\n",
    "\n",
    "x = ndf[ ['pclass', 'age' ,'sibsp', 'parch' ,'female' ,'male', 'C' ,'Q' ,'S', 'child_women'] ]\n",
    "y = ndf['survived'] # 종속변수\n",
    "\n",
    "from sklearn import preprocessing\n",
    "X = preprocessing.StandardScaler().fit(x).transform(x)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(x,y,test_size = 0.3, random_state = 33)\n",
    "\n",
    "from  sklearn.ensemble   import  RandomForestClassifier \n",
    "tree_model = RandomForestClassifier( n_estimators=100, oob_score=True, random_state= 9 ).fit(X_train, y_train)\n",
    "# n_estimator : 생성할 트리의 개수. 약한 학습자 100개 생성. 수가 많아지면 시간이 오래걸림\n",
    "# oob_score : out of bag 기능의 사용여부. 훈련이 끝난 후 자동으로 oob 평가를 수행. 평가를 보고싶으면 아래의 print를 수행\n",
    "            # out of bag : 100개의 tree가 훈련 데이터를 사용할 때 63%만 사용, 나머지 37%의 oob sample로 평가. 앙상블의 평가를 oob 평가를 평균하여 획득\n",
    "print ( tree_model.oob_score_)\n",
    "\n",
    "y_hat = tree_model.predict( X_test )\n",
    "\n",
    "from sklearn import metrics\n",
    "randomforest_matrix = metrics.confusion_matrix( y_test, y_hat )\n",
    "print( randomforest_matrix )\n",
    "\n",
    "tn, fp, fn, tp = metrics.confusion_matrix( y_test, y_hat ).ravel()\n",
    "f1_report = metrics.classification_report( y_test, y_hat )\n",
    "print( f1_report )\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score( y_test, y_hat)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ※ 문제235. 아까 점심시간 문제처럼 지금 수행하고 있는 Random Forest 모델의 최적의 하이퍼 파라미터를 for loop문으로 찾는데 oob_score=False로 수행\n",
    "    알아야 할 파라미터\n",
    "        1. 훈련 데이터와 테스트 데이터 나눌 때 random_state\n",
    "        2. RandomForestClassifier 모델 생성시 random_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T05:20:53.077669Z",
     "start_time": "2020-07-27T05:12:31.773497Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_random_state:  47 model_random_state:  26 accuracy: 0.8372093023255814\n"
     ]
    }
   ],
   "source": [
    "# 예제 : seaborn 타이타닉 데이터로 Random Forest 모델 생성\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "pd.set_option('display.max_columns',15)\n",
    "\n",
    "df = sns.load_dataset('titanic')\n",
    "\n",
    "mask4 = (df.age<10) | (df.sex=='female') \n",
    "df['child_women'] = mask4.astype(int)\n",
    "\n",
    "rdf = df.drop(['deck','embark_town'], axis =1)\n",
    "rdf = rdf.dropna( subset=['age'], how='any', axis=0)\n",
    "\n",
    "most_freq = rdf['embarked'].value_counts().idxmax()\n",
    "rdf['embarked'].fillna(most_freq, inplace = True)\n",
    "\n",
    "ndf = rdf[['survived','pclass','sex','age','sibsp','parch','embarked','child_women']]\n",
    "\n",
    "gender = pd.get_dummies(ndf['sex'])\n",
    "ndf = pd.concat([ndf,gender], axis= 1)\n",
    "onehot_embarked = pd.get_dummies(ndf['embarked'])\n",
    "ndf = pd.concat([ndf,onehot_embarked],axis=1)\n",
    "ndf.drop(['sex','embarked'], axis=1, inplace = True)\n",
    "\n",
    "x = ndf[ ['pclass', 'age' ,'sibsp', 'parch' ,'female' ,'male', 'C' ,'Q' ,'S', 'child_women'] ]\n",
    "y = ndf['survived'] # 종속변수\n",
    "\n",
    "from sklearn import preprocessing\n",
    "X = preprocessing.StandardScaler().fit(x).transform(x)\n",
    "\n",
    "b=[]\n",
    "c=[]\n",
    "for i in range(1, 50):\n",
    "    X_train,X_test,y_train,y_test = train_test_split(x,y,test_size = 0.3, random_state = i)\n",
    "    \n",
    "    for k in range(1,50):\n",
    "        b.append((i,k))\n",
    "        \n",
    "        from  sklearn.ensemble   import  RandomForestClassifier \n",
    "        tree_model = RandomForestClassifier( n_estimators=100, oob_score=False, random_state= k ).fit(X_train, y_train)\n",
    "                \n",
    "        y_hat = tree_model.predict( X_test )\n",
    "        \n",
    "        from sklearn import metrics\n",
    "        randomforest_matrix = metrics.confusion_matrix( y_test, y_hat )\n",
    "\n",
    "        tn, fp, fn, tp = metrics.confusion_matrix( y_test, y_hat ).ravel()\n",
    "        f1_report = metrics.classification_report( y_test, y_hat )\n",
    "        \n",
    "        from sklearn.metrics import accuracy_score\n",
    "        accuracy = accuracy_score( y_test, y_hat)\n",
    "        c.append(accuracy)\n",
    "\n",
    "idx = c.index(np.max(c))\n",
    "print('data_random_state: ', b[idx][0], 'model_random_state: ', b[idx][1], 'accuracy:', c[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>■ 랜덤포레스트 모델의 성능을 올리기 위해 데이터 전처리 하는 방법</b>\n",
    "    1. 결측치가 많은 컬럼은 삭제 - cabin\n",
    "    2. 나이의 결측치를 치환 - 최빈값으로 치환\n",
    "        - Kaggle : 정확도 0.80861, 상위 6%\n",
    "    3. 이상치 제거 - fare의 이상치를 제거\n",
    "        - Kaggle : 정확도 0.81339, 상위 5%\n",
    "        \n",
    "    * 파생변수 생성\n",
    "        1. 이름의 호칭을 나이 결측치를 채워넣는다 - 상위 4%\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T05:27:26.110137Z",
     "start_time": "2020-07-27T05:27:25.740364Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[105  16]\n",
      " [ 20  74]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.87      0.85       121\n",
      "           1       0.82      0.79      0.80        94\n",
      "\n",
      "    accuracy                           0.83       215\n",
      "   macro avg       0.83      0.83      0.83       215\n",
      "weighted avg       0.83      0.83      0.83       215\n",
      "\n",
      "0.8325581395348837\n"
     ]
    }
   ],
   "source": [
    "# 예제 : seaborn 타이타닉 데이터로 Random Forest 모델 생성\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "df = sns.load_dataset('titanic')\n",
    "\n",
    "pd.set_option('display.max_columns',15)\n",
    "\n",
    "mask4 = (df.age<10) | (df.sex=='female') \n",
    "df['child_women'] = mask4.astype(int)\n",
    "\n",
    "rdf = df.drop(['deck','embark_town'], axis =1)\n",
    "rdf = rdf.dropna( subset=['age'], how='any', axis=0)\n",
    "\n",
    "most_freq = rdf['embarked'].value_counts().idxmax()\n",
    "rdf['embarked'].fillna(most_freq, inplace = True)\n",
    "\n",
    "ndf = rdf[['survived','pclass','sex','age','sibsp','parch','embarked','child_women']]\n",
    "\n",
    "gender = pd.get_dummies(ndf['sex'])\n",
    "ndf = pd.concat([ndf,gender], axis= 1)\n",
    "onehot_embarked = pd.get_dummies(ndf['embarked'])\n",
    "ndf = pd.concat([ndf,onehot_embarked],axis=1)\n",
    "ndf.drop(['sex','embarked'], axis=1, inplace = True)\n",
    "\n",
    "x = ndf[ ['pclass', 'age' ,'sibsp', 'parch' ,'female' ,'male', 'C' ,'Q' ,'S', 'child_women'] ]\n",
    "y = ndf['survived'] # 종속변수\n",
    "\n",
    "from sklearn import preprocessing\n",
    "X = preprocessing.StandardScaler().fit(x).transform(x)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(x,y,test_size = 0.3, random_state = 47)\n",
    "\n",
    "from  sklearn.ensemble   import  RandomForestClassifier \n",
    "tree_model = RandomForestClassifier( n_estimators=200, oob_score=False, random_state= 26).fit(X_train, y_train)\n",
    "# print ( tree_model.oob_score_)\n",
    "\n",
    "y_hat = tree_model.predict( X_test )\n",
    "\n",
    "from sklearn import metrics\n",
    "randomforest_matrix = metrics.confusion_matrix( y_test, y_hat )\n",
    "print( randomforest_matrix )\n",
    "\n",
    "tn, fp, fn, tp = metrics.confusion_matrix( y_test, y_hat ).ravel()\n",
    "f1_report = metrics.classification_report( y_test, y_hat )\n",
    "print( f1_report )\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score( y_test, y_hat)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ※ 문제236. seaborn 타이타닉에서 결측치가 가장 많은 컬럼인 cabin을 삭제하시오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T05:29:56.132539Z",
     "start_time": "2020-07-27T05:29:56.090564Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>embarked</th>\n",
       "      <th>class</th>\n",
       "      <th>who</th>\n",
       "      <th>adult_male</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>alive</th>\n",
       "      <th>alone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>S</td>\n",
       "      <td>Second</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>S</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>C</td>\n",
       "      <td>First</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>Q</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>Queenstown</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     survived  pclass     sex   age  sibsp  parch     fare embarked   class  \\\n",
       "0           0       3    male  22.0      1      0   7.2500        S   Third   \n",
       "1           1       1  female  38.0      1      0  71.2833        C   First   \n",
       "2           1       3  female  26.0      0      0   7.9250        S   Third   \n",
       "3           1       1  female  35.0      1      0  53.1000        S   First   \n",
       "4           0       3    male  35.0      0      0   8.0500        S   Third   \n",
       "..        ...     ...     ...   ...    ...    ...      ...      ...     ...   \n",
       "886         0       2    male  27.0      0      0  13.0000        S  Second   \n",
       "887         1       1  female  19.0      0      0  30.0000        S   First   \n",
       "888         0       3  female   NaN      1      2  23.4500        S   Third   \n",
       "889         1       1    male  26.0      0      0  30.0000        C   First   \n",
       "890         0       3    male  32.0      0      0   7.7500        Q   Third   \n",
       "\n",
       "       who  adult_male  embark_town alive  alone  \n",
       "0      man        True  Southampton    no  False  \n",
       "1    woman       False    Cherbourg   yes  False  \n",
       "2    woman       False  Southampton   yes   True  \n",
       "3    woman       False  Southampton   yes  False  \n",
       "4      man        True  Southampton    no   True  \n",
       "..     ...         ...          ...   ...    ...  \n",
       "886    man        True  Southampton    no   True  \n",
       "887  woman       False  Southampton   yes   True  \n",
       "888  woman       False  Southampton    no  False  \n",
       "889    man        True    Cherbourg   yes   True  \n",
       "890    man        True   Queenstown    no   True  \n",
       "\n",
       "[891 rows x 14 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예제 : seaborn 타이타닉 데이터로 Random Forest 모델 생성\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "pd.set_option('display.max_columns',15)\n",
    "\n",
    "df = sns.load_dataset('titanic')\n",
    "df = df.drop(['deck'],axis=1,)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ※ 문제237. seaborn 타이타닉의 나이의 결측치를 최빈값으로 치환하시오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T06:11:36.629421Z",
     "start_time": "2020-07-27T06:11:36.249655Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[152  14]\n",
      " [ 29  73]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.92      0.88       166\n",
      "           1       0.84      0.72      0.77       102\n",
      "\n",
      "    accuracy                           0.84       268\n",
      "   macro avg       0.84      0.82      0.82       268\n",
      "weighted avg       0.84      0.84      0.84       268\n",
      "\n",
      "0.8395522388059702\n"
     ]
    }
   ],
   "source": [
    "# 예제 : seaborn 타이타닉 데이터로 Random Forest 모델 생성\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "pd.set_option('display.max_columns',15)\n",
    "\n",
    "df = sns.load_dataset('titanic')\n",
    "\n",
    "mask4 = (df.age<10) | (df.sex=='female') \n",
    "df['child_women'] = mask4.astype(int)\n",
    "\n",
    "rdf = df.drop(['deck','embark_town'], axis =1)\n",
    "\n",
    "most_freq = df['age'].value_counts(dropna=True).idxmax()\n",
    "rdf['age'].fillna(most_freq, inplace=True)\n",
    "\n",
    "most_freq = rdf['embarked'].value_counts().idxmax()\n",
    "rdf['embarked'].fillna(most_freq, inplace = True)\n",
    "\n",
    "ndf = rdf[['survived','pclass','sex','age','sibsp','parch','embarked','child_women']]\n",
    "\n",
    "gender = pd.get_dummies(ndf['sex'])\n",
    "ndf = pd.concat([ndf,gender], axis= 1)\n",
    "onehot_embarked = pd.get_dummies(ndf['embarked'])\n",
    "ndf = pd.concat([ndf,onehot_embarked],axis=1)\n",
    "ndf.drop(['sex','embarked'], axis=1, inplace = True)\n",
    "\n",
    "x = ndf[ ['pclass', 'age' ,'sibsp', 'parch' ,'female' ,'male', 'C' ,'Q' ,'S', 'child_women'] ]\n",
    "y = ndf['survived'] # 종속변수\n",
    "\n",
    "from sklearn import preprocessing\n",
    "X = preprocessing.StandardScaler().fit(x).transform(x)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(x,y,test_size = 0.3, random_state = 47)\n",
    "\n",
    "from  sklearn.ensemble   import  RandomForestClassifier \n",
    "tree_model = RandomForestClassifier( n_estimators=200, oob_score=True, random_state= 26).fit(X_train, y_train)\n",
    "# print ( tree_model.oob_score_)\n",
    "\n",
    "y_hat = tree_model.predict( X_test )\n",
    "\n",
    "from sklearn import metrics\n",
    "randomforest_matrix = metrics.confusion_matrix( y_test, y_hat )\n",
    "print( randomforest_matrix )\n",
    "\n",
    "tn, fp, fn, tp = metrics.confusion_matrix( y_test, y_hat ).ravel()\n",
    "f1_report = metrics.classification_report( y_test, y_hat )\n",
    "print( f1_report )\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score( y_test, y_hat)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T05:35:04.352781Z",
     "start_time": "2020-07-27T05:35:03.968018Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[142  22]\n",
      " [ 29  75]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.87      0.85       164\n",
      "           1       0.77      0.72      0.75       104\n",
      "\n",
      "    accuracy                           0.81       268\n",
      "   macro avg       0.80      0.79      0.80       268\n",
      "weighted avg       0.81      0.81      0.81       268\n",
      "\n",
      "0.8097014925373134\n"
     ]
    }
   ],
   "source": [
    "# 예제 : seaborn 타이타닉 데이터로 Random Forest 모델 생성\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "pd.set_option('display.max_columns',15)\n",
    "\n",
    "df = sns.load_dataset('titanic')\n",
    "\n",
    "mask4 = (df.age<10) | (df.sex=='female') \n",
    "df['child_women'] = mask4.astype(int)\n",
    "\n",
    "rdf = df.drop(['deck','embark_town'], axis =1)\n",
    "\n",
    "most_freq = df['age'].value_counts(dropna=True).idxmax()\n",
    "rdf['age'].fillna(most_freq, inplace=True)\n",
    "\n",
    "most_freq = rdf['embarked'].value_counts().idxmax()\n",
    "rdf['embarked'].fillna(most_freq, inplace = True)\n",
    "\n",
    "ndf = rdf[['survived','pclass','sex','age','sibsp','parch','embarked','child_women']]\n",
    "\n",
    "gender = pd.get_dummies(ndf['sex'])\n",
    "ndf = pd.concat([ndf,gender], axis= 1)\n",
    "onehot_embarked = pd.get_dummies(ndf['embarked'])\n",
    "ndf = pd.concat([ndf,onehot_embarked],axis=1)\n",
    "ndf.drop(['sex','embarked'], axis=1, inplace = True)\n",
    "\n",
    "x = ndf[ ['pclass', 'age' ,'sibsp', 'parch' ,'female' ,'male', 'C' ,'Q' ,'S', 'child_women'] ]\n",
    "y = ndf['survived'] # 종속변수\n",
    "\n",
    "from sklearn import preprocessing\n",
    "X = preprocessing.StandardScaler().fit(x).transform(x)\n",
    "\n",
    "b=[]\n",
    "c=[]\n",
    "for i in range(1, 50):\n",
    "    X_train,X_test,y_train,y_test = train_test_split(x,y,test_size = 0.3, random_state = i)\n",
    "    \n",
    "    for k in range(1,50):\n",
    "        b.append((i,k))\n",
    "        \n",
    "        from  sklearn.ensemble   import  RandomForestClassifier \n",
    "        tree_model = RandomForestClassifier( n_estimators=100, oob_score=False, random_state= k ).fit(X_train, y_train)\n",
    "                \n",
    "        y_hat = tree_model.predict( X_test )\n",
    "        \n",
    "        from sklearn import metrics\n",
    "        randomforest_matrix = metrics.confusion_matrix( y_test, y_hat )\n",
    "\n",
    "        tn, fp, fn, tp = metrics.confusion_matrix( y_test, y_hat ).ravel()\n",
    "        f1_report = metrics.classification_report( y_test, y_hat )\n",
    "        \n",
    "        from sklearn.metrics import accuracy_score\n",
    "        accuracy = accuracy_score( y_test, y_hat)\n",
    "        c.append(accuracy)\n",
    "\n",
    "idx = c.index(np.max(c))\n",
    "print('data_random_state: ', b[idx][0], 'model_random_state: ', b[idx][1], 'accuracy:', c[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ※ 문제238. 운임(fare)의 이상치를 제거하시오\n",
    "    이상치 = (운임 > 운임평균 + 5*표준편차)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T06:21:16.519760Z",
     "start_time": "2020-07-27T06:14:01.697178Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_random_state:  6 model_random_state:  11 accuracy: 0.8679245283018868\n"
     ]
    }
   ],
   "source": [
    "# 예제 : seaborn 타이타닉 데이터로 Random Forest 모델 생성\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "pd.set_option('display.max_columns',15)\n",
    "\n",
    "df = sns.load_dataset('titanic')\n",
    "\n",
    "mask4 = (df.age<10) | (df.sex=='female') \n",
    "df['child_women'] = mask4.astype(int)\n",
    "\n",
    "rdf = df.drop(['deck','embark_town'], axis =1)\n",
    "\n",
    "most_freq = df['age'].value_counts(dropna=True).idxmax()\n",
    "rdf['age'].fillna(most_freq, inplace=True)\n",
    "\n",
    "most_freq = rdf['embarked'].value_counts().idxmax()\n",
    "rdf['embarked'].fillna(most_freq, inplace = True)\n",
    "\n",
    "local_std = df.fare.std()*5\n",
    "res = df['fare'][df.fare>local_std]\n",
    "rdf = rdf[rdf.fare < local_std]\n",
    "\n",
    "ndf = rdf[['survived','pclass','sex','age','sibsp','parch','embarked','child_women','fare']]\n",
    "\n",
    "gender = pd.get_dummies(ndf['sex'])\n",
    "ndf = pd.concat([ndf,gender], axis= 1)\n",
    "onehot_embarked = pd.get_dummies(ndf['embarked'])\n",
    "ndf = pd.concat([ndf,onehot_embarked],axis=1)\n",
    "ndf.drop(['sex','embarked'], axis=1, inplace = True)\n",
    "\n",
    "x = ndf[ ['pclass', 'age' ,'sibsp', 'parch' ,'female' ,'male', 'C' ,'Q' ,'S', 'child_women','fare'] ]\n",
    "y = ndf['survived'] # 종속변수\n",
    "\n",
    "from sklearn import preprocessing\n",
    "X = preprocessing.StandardScaler().fit(x).transform(x)\n",
    "\n",
    "b=[]\n",
    "c=[]\n",
    "for i in range(1, 50):\n",
    "    X_train,X_test,y_train,y_test = train_test_split(x,y,test_size = 0.3, random_state = i)\n",
    "    \n",
    "    for k in range(1,50):\n",
    "        b.append((i,k))\n",
    "        \n",
    "        from  sklearn.ensemble   import  RandomForestClassifier \n",
    "        tree_model = RandomForestClassifier( n_estimators=100, oob_score=False, random_state= k ).fit(X_train, y_train)\n",
    "                \n",
    "        y_hat = tree_model.predict( X_test )\n",
    "        \n",
    "        from sklearn import metrics\n",
    "        randomforest_matrix = metrics.confusion_matrix( y_test, y_hat )\n",
    "\n",
    "        tn, fp, fn, tp = metrics.confusion_matrix( y_test, y_hat ).ravel()\n",
    "        f1_report = metrics.classification_report( y_test, y_hat )\n",
    "        \n",
    "        from sklearn.metrics import accuracy_score\n",
    "        accuracy = accuracy_score( y_test, y_hat)\n",
    "        c.append(accuracy)\n",
    "\n",
    "idx = c.index(np.max(c))\n",
    "print('data_random_state: ', b[idx][0], 'model_random_state: ', b[idx][1], 'accuracy:', c[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T06:13:34.339665Z",
     "start_time": "2020-07-27T06:13:33.867956Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[148  19]\n",
      " [ 39  59]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.89      0.84       167\n",
      "           1       0.76      0.60      0.67        98\n",
      "\n",
      "    accuracy                           0.78       265\n",
      "   macro avg       0.77      0.74      0.75       265\n",
      "weighted avg       0.78      0.78      0.77       265\n",
      "\n",
      "0.7811320754716982\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "pd.set_option('display.max_columns',15)\n",
    "\n",
    "df = sns.load_dataset('titanic')\n",
    "\n",
    "mask4 = (df.age<10) | (df.sex=='female') \n",
    "df['child_women'] = mask4.astype(int)\n",
    "\n",
    "rdf = df.drop(['deck','embark_town'], axis =1)\n",
    "\n",
    "most_freq = df['age'].value_counts(dropna=True).idxmax()\n",
    "rdf['age'].fillna(most_freq, inplace=True)\n",
    "\n",
    "most_freq = rdf['embarked'].value_counts().idxmax()\n",
    "rdf['embarked'].fillna(most_freq, inplace = True)\n",
    "\n",
    "local_std = rdf.fare.std()*5\n",
    "res = rdf['fare'][df.fare>local_std]\n",
    "rdf = rdf[rdf.fare < local_std]\n",
    "\n",
    "ndf = rdf[['survived','pclass','sex','age','sibsp','parch','embarked','child_women','fare']]\n",
    "\n",
    "gender = pd.get_dummies(ndf['sex'])\n",
    "ndf = pd.concat([ndf,gender], axis= 1)\n",
    "onehot_embarked = pd.get_dummies(ndf['embarked'])\n",
    "ndf = pd.concat([ndf,onehot_embarked],axis=1)\n",
    "ndf.drop(['sex','embarked'], axis=1, inplace = True)\n",
    "\n",
    "x = ndf[ ['pclass', 'age' ,'sibsp', 'parch' ,'female' ,'male', 'C' ,'Q' ,'S', 'child_women','fare'] ]\n",
    "y = ndf['survived'] # 종속변수\n",
    "\n",
    "from sklearn import preprocessing\n",
    "X = preprocessing.StandardScaler().fit(x).transform(x)\n",
    "\n",
    "from  sklearn.ensemble   import  RandomForestClassifier \n",
    "tree_model = RandomForestClassifier( n_estimators=100, oob_score=False, random_state= 37).fit(X_train, y_train)\n",
    "# print ( tree_model.oob_score_)\n",
    "\n",
    "y_hat = tree_model.predict( X_test )\n",
    "\n",
    "from sklearn import metrics\n",
    "randomforest_matrix = metrics.confusion_matrix( y_test, y_hat )\n",
    "print( randomforest_matrix )\n",
    "\n",
    "tn, fp, fn, tp = metrics.confusion_matrix( y_test, y_hat ).ravel()\n",
    "f1_report = metrics.classification_report( y_test, y_hat )\n",
    "print( f1_report )\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score( y_test, y_hat)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ※ 문제239. Kaggle의 타이타닉 데이터를 랜덤포레스트로 모델을 생성하고 결과를 Kaggle에 제출해서 순위를 확인하시오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T06:32:48.969280Z",
     "start_time": "2020-07-27T06:32:48.507565Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "892,0\n",
      "893,0\n",
      "894,0\n",
      "895,0\n",
      "896,0\n",
      "897,0\n",
      "898,0\n",
      "899,0\n",
      "900,1\n",
      "901,0\n",
      "902,0\n",
      "903,0\n",
      "904,1\n",
      "905,0\n",
      "906,1\n",
      "907,1\n",
      "908,0\n",
      "909,0\n",
      "910,0\n",
      "911,1\n",
      "912,1\n",
      "913,1\n",
      "914,1\n",
      "915,0\n",
      "916,1\n",
      "917,0\n",
      "918,1\n",
      "919,0\n",
      "920,0\n",
      "921,0\n",
      "922,0\n",
      "923,0\n",
      "924,1\n",
      "925,0\n",
      "926,0\n",
      "927,0\n",
      "928,0\n",
      "929,0\n",
      "930,0\n",
      "931,0\n",
      "932,0\n",
      "933,0\n",
      "934,0\n",
      "935,1\n",
      "936,1\n",
      "937,0\n",
      "938,0\n",
      "939,0\n",
      "940,1\n",
      "941,1\n",
      "942,0\n",
      "943,0\n",
      "944,1\n",
      "945,0\n",
      "946,0\n",
      "947,0\n",
      "948,0\n",
      "949,0\n",
      "950,0\n",
      "951,1\n",
      "952,0\n",
      "953,0\n",
      "954,0\n",
      "955,0\n",
      "956,1\n",
      "957,1\n",
      "958,1\n",
      "959,0\n",
      "960,0\n",
      "961,1\n",
      "962,0\n",
      "963,0\n",
      "964,1\n",
      "965,0\n",
      "966,1\n",
      "967,0\n",
      "968,0\n",
      "969,1\n",
      "970,0\n",
      "971,0\n",
      "972,1\n",
      "973,0\n",
      "974,0\n",
      "975,0\n",
      "976,0\n",
      "977,0\n",
      "978,0\n",
      "979,1\n",
      "980,0\n",
      "981,1\n",
      "982,0\n",
      "983,0\n",
      "984,1\n",
      "985,0\n",
      "986,0\n",
      "987,0\n",
      "988,1\n",
      "989,0\n",
      "990,0\n",
      "991,0\n",
      "992,1\n",
      "993,0\n",
      "994,0\n",
      "995,0\n",
      "996,1\n",
      "997,0\n",
      "998,0\n",
      "999,0\n",
      "1000,0\n",
      "1001,0\n",
      "1002,0\n",
      "1003,0\n",
      "1004,1\n",
      "1005,1\n",
      "1006,1\n",
      "1007,0\n",
      "1008,0\n",
      "1009,1\n",
      "1010,0\n",
      "1011,1\n",
      "1012,1\n",
      "1013,0\n",
      "1014,1\n",
      "1015,0\n",
      "1016,0\n",
      "1017,1\n",
      "1018,0\n",
      "1019,1\n",
      "1020,0\n",
      "1021,0\n",
      "1022,0\n",
      "1023,0\n",
      "1024,0\n",
      "1025,0\n",
      "1026,0\n",
      "1027,0\n",
      "1028,0\n",
      "1029,0\n",
      "1030,0\n",
      "1031,0\n",
      "1032,0\n",
      "1033,1\n",
      "1034,0\n",
      "1035,0\n",
      "1036,0\n",
      "1037,0\n",
      "1038,0\n",
      "1039,0\n",
      "1040,0\n",
      "1041,0\n",
      "1042,1\n",
      "1043,0\n",
      "1044,0\n",
      "1045,1\n",
      "1046,0\n",
      "1047,0\n",
      "1048,1\n",
      "1049,0\n",
      "1050,0\n",
      "1051,1\n",
      "1052,0\n",
      "1053,1\n",
      "1054,1\n",
      "1055,0\n",
      "1056,0\n",
      "1057,1\n",
      "1058,0\n",
      "1059,0\n",
      "1060,1\n",
      "1061,0\n",
      "1062,0\n",
      "1063,0\n",
      "1064,0\n",
      "1065,0\n",
      "1066,0\n",
      "1067,1\n",
      "1068,1\n",
      "1069,1\n",
      "1070,1\n",
      "1071,1\n",
      "1072,0\n",
      "1073,0\n",
      "1074,1\n",
      "1075,0\n",
      "1076,1\n",
      "1077,0\n",
      "1078,1\n",
      "1079,0\n",
      "1080,0\n",
      "1081,0\n",
      "1082,0\n",
      "1083,0\n",
      "1084,0\n",
      "1085,0\n",
      "1086,1\n",
      "1087,0\n",
      "1088,1\n",
      "1089,1\n",
      "1090,0\n",
      "1091,0\n",
      "1092,1\n",
      "1093,1\n",
      "1094,0\n",
      "1095,1\n",
      "1096,0\n",
      "1097,0\n",
      "1098,0\n",
      "1099,0\n",
      "1100,1\n",
      "1101,0\n",
      "1102,0\n",
      "1103,0\n",
      "1104,0\n",
      "1105,1\n",
      "1106,1\n",
      "1107,0\n",
      "1108,0\n",
      "1109,0\n",
      "1110,1\n",
      "1111,0\n",
      "1112,1\n",
      "1113,0\n",
      "1114,1\n",
      "1115,0\n",
      "1116,1\n",
      "1117,1\n",
      "1118,0\n",
      "1119,0\n",
      "1120,0\n",
      "1121,0\n",
      "1122,0\n",
      "1123,1\n",
      "1124,0\n",
      "1125,0\n",
      "1126,1\n",
      "1127,0\n",
      "1128,1\n",
      "1129,0\n",
      "1130,1\n",
      "1131,1\n",
      "1132,1\n",
      "1133,1\n",
      "1134,1\n",
      "1135,0\n",
      "1136,0\n",
      "1137,0\n",
      "1138,1\n",
      "1139,0\n",
      "1140,1\n",
      "1141,0\n",
      "1142,1\n",
      "1143,0\n",
      "1144,1\n",
      "1145,0\n",
      "1146,0\n",
      "1147,0\n",
      "1148,0\n",
      "1149,0\n",
      "1150,1\n",
      "1151,0\n",
      "1152,0\n",
      "1153,0\n",
      "1154,1\n",
      "1155,1\n",
      "1156,0\n",
      "1157,0\n",
      "1158,0\n",
      "1159,0\n",
      "1160,0\n",
      "1161,0\n",
      "1162,0\n",
      "1163,0\n",
      "1164,1\n",
      "1165,1\n",
      "1166,0\n",
      "1167,1\n",
      "1168,0\n",
      "1169,0\n",
      "1170,0\n",
      "1171,0\n",
      "1172,0\n",
      "1173,1\n",
      "1174,0\n",
      "1175,1\n",
      "1176,1\n",
      "1177,0\n",
      "1178,0\n",
      "1179,0\n",
      "1180,0\n",
      "1181,0\n",
      "1182,0\n",
      "1183,0\n",
      "1184,0\n",
      "1185,0\n",
      "1186,0\n",
      "1187,0\n",
      "1188,1\n",
      "1189,0\n",
      "1190,0\n",
      "1191,0\n",
      "1192,0\n",
      "1193,0\n",
      "1194,0\n",
      "1195,0\n",
      "1196,0\n",
      "1197,1\n",
      "1198,1\n",
      "1199,1\n",
      "1200,0\n",
      "1201,0\n",
      "1202,0\n",
      "1203,0\n",
      "1204,0\n",
      "1205,0\n",
      "1206,1\n",
      "1207,1\n",
      "1208,0\n",
      "1209,0\n",
      "1210,0\n",
      "1211,0\n",
      "1212,0\n",
      "1213,0\n",
      "1214,0\n",
      "1215,0\n",
      "1216,1\n",
      "1217,0\n",
      "1218,1\n",
      "1219,0\n",
      "1220,0\n",
      "1221,0\n",
      "1222,1\n",
      "1223,0\n",
      "1224,0\n",
      "1225,1\n",
      "1226,0\n",
      "1227,0\n",
      "1228,0\n",
      "1229,0\n",
      "1230,0\n",
      "1231,0\n",
      "1232,0\n",
      "1233,0\n",
      "1234,0\n",
      "1235,1\n",
      "1236,0\n",
      "1237,0\n",
      "1238,0\n",
      "1239,1\n",
      "1240,0\n",
      "1241,1\n",
      "1242,1\n",
      "1243,0\n",
      "1244,0\n",
      "1245,0\n",
      "1246,1\n",
      "1247,0\n",
      "1248,1\n",
      "1249,0\n",
      "1250,0\n",
      "1251,0\n",
      "1252,0\n",
      "1253,1\n",
      "1254,1\n",
      "1255,0\n",
      "1256,1\n",
      "1257,0\n",
      "1258,0\n",
      "1259,0\n",
      "1260,1\n",
      "1261,0\n",
      "1262,0\n",
      "1263,1\n",
      "1264,0\n",
      "1265,0\n",
      "1266,1\n",
      "1267,1\n",
      "1268,0\n",
      "1269,0\n",
      "1270,0\n",
      "1271,0\n",
      "1272,0\n",
      "1273,0\n",
      "1274,1\n",
      "1275,1\n",
      "1276,0\n",
      "1277,1\n",
      "1278,0\n",
      "1279,0\n",
      "1280,0\n",
      "1281,0\n",
      "1282,0\n",
      "1283,1\n",
      "1284,0\n",
      "1285,0\n",
      "1286,0\n",
      "1287,1\n",
      "1288,0\n",
      "1289,1\n",
      "1290,0\n",
      "1291,0\n",
      "1292,1\n",
      "1293,0\n",
      "1294,1\n",
      "1295,1\n",
      "1296,0\n",
      "1297,0\n",
      "1298,0\n",
      "1299,0\n",
      "1300,0\n",
      "1301,1\n",
      "1302,0\n",
      "1303,1\n",
      "1304,1\n",
      "1305,0\n",
      "1306,1\n",
      "1307,0\n",
      "1308,0\n",
      "1309,1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "pd.set_option('display.max_columns', 15)\n",
    "\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "mask4 = (df.Age < 10) | (df.Sex == 'female')\n",
    "df['child_women'] = mask4.astype(int)\n",
    "\n",
    "rdf = df.drop(['PassengerId', 'Cabin', 'Name', 'Ticket'], axis=1)\n",
    "\n",
    "most_freq = rdf['Age'].value_counts(dropna=True).idxmax()\n",
    "rdf['Age'].fillna(most_freq, inplace=True)\n",
    "\n",
    "most_freq = rdf['Embarked'].value_counts().idxmax()\n",
    "rdf['Embarked'].fillna(most_freq, inplace=True)\n",
    "\n",
    "local_std = rdf.Fare.std() * 5\n",
    "rdf = rdf[:][rdf['Fare'] < local_std]\n",
    "ndf = rdf\n",
    "\n",
    "gender = pd.get_dummies(ndf['Sex'])\n",
    "ndf = pd.concat([ndf, gender], axis=1)\n",
    "onehot_embarked = pd.get_dummies(ndf['Embarked'])\n",
    "ndf = pd.concat([ndf, onehot_embarked], axis=1)\n",
    "ndf.drop(['Sex', 'Embarked'], axis=1, inplace=True)\n",
    "\n",
    "x = ndf[['Fare', 'Pclass', 'Age', 'SibSp', 'Parch',\n",
    "         'female', 'male', 'C', 'Q', 'S', 'child_women']]\n",
    "y = ndf['Survived']  # 종속변수\n",
    "\n",
    "X = preprocessing.StandardScaler().fit(x).transform(x)\n",
    "\n",
    "\"\"\"\n",
    "test data\n",
    "\"\"\"\n",
    "\n",
    "x_ktest = pd.read_csv(\"test.csv\")\n",
    "mask4 = (x_ktest.Age < 10) | (x_ktest.Sex == 'female')\n",
    "x_ktest['child_women'] = mask4.astype(int)\n",
    "\n",
    "rdf_x_ktest = x_ktest.drop(['PassengerId', 'Cabin', 'Name', 'Ticket'], axis=1)\n",
    "\n",
    "most_freq = rdf_x_ktest['Age'].value_counts(dropna=True).idxmax()\n",
    "rdf_x_ktest['Age'].fillna(most_freq, inplace=True)\n",
    "\n",
    "most_freq = rdf_x_ktest['Embarked'].value_counts().idxmax()\n",
    "rdf_x_ktest['Embarked'].fillna(most_freq, inplace=True)\n",
    "\n",
    "most_freq = rdf_x_ktest['Fare'].value_counts().idxmax()\n",
    "rdf_x_ktest['Fare'].fillna(most_freq, inplace=True)\n",
    "\n",
    "ndf_x_ktest = rdf_x_ktest\n",
    "\n",
    "gender = pd.get_dummies(ndf_x_ktest['Sex'])\n",
    "ndf_x_ktest = pd.concat([ndf_x_ktest, gender], axis=1)\n",
    "onehot_embarked = pd.get_dummies(ndf_x_ktest['Embarked'])\n",
    "ndf_x_ktest = pd.concat([ndf_x_ktest, onehot_embarked], axis=1)\n",
    "ndf_x_ktest.drop(['Sex', 'Embarked'], axis=1, inplace=True)\n",
    "\n",
    "x = ndf_x_ktest[['Fare', 'Pclass', 'Age', 'SibSp', 'Parch','female', 'male', 'C', 'Q', 'S', 'child_women']]\n",
    "\n",
    "X_test = preprocessing.StandardScaler().fit(x).transform(x)\n",
    "\n",
    "tree_model = RandomForestClassifier(n_estimators=200, oob_score=False, random_state=14).fit(X, y)\n",
    "\n",
    "y_hat = tree_model.predict(X_test)\n",
    "\n",
    "for i, a in enumerate(y_hat):\n",
    "    print(str(i+892) + ',' + str(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ※ 문제240. 나이의 결측치를 최빈값이 아니라 호칭의 평균나이로 채워넣고 다시 학습시켜 Kaggle에 올리시오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T07:17:51.063243Z",
     "start_time": "2020-07-27T07:17:50.165795Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "892,0\n",
      "893,0\n",
      "894,0\n",
      "895,0\n",
      "896,0\n",
      "897,0\n",
      "898,0\n",
      "899,0\n",
      "900,1\n",
      "901,0\n",
      "902,0\n",
      "903,0\n",
      "904,1\n",
      "905,0\n",
      "906,1\n",
      "907,1\n",
      "908,0\n",
      "909,1\n",
      "910,0\n",
      "911,1\n",
      "912,1\n",
      "913,1\n",
      "914,1\n",
      "915,1\n",
      "916,1\n",
      "917,0\n",
      "918,1\n",
      "919,0\n",
      "920,0\n",
      "921,0\n",
      "922,0\n",
      "923,0\n",
      "924,1\n",
      "925,0\n",
      "926,0\n",
      "927,1\n",
      "928,0\n",
      "929,0\n",
      "930,0\n",
      "931,1\n",
      "932,0\n",
      "933,0\n",
      "934,0\n",
      "935,1\n",
      "936,1\n",
      "937,0\n",
      "938,0\n",
      "939,0\n",
      "940,1\n",
      "941,1\n",
      "942,0\n",
      "943,0\n",
      "944,1\n",
      "945,0\n",
      "946,0\n",
      "947,0\n",
      "948,0\n",
      "949,0\n",
      "950,0\n",
      "951,1\n",
      "952,0\n",
      "953,0\n",
      "954,0\n",
      "955,0\n",
      "956,1\n",
      "957,1\n",
      "958,1\n",
      "959,0\n",
      "960,0\n",
      "961,1\n",
      "962,0\n",
      "963,0\n",
      "964,1\n",
      "965,0\n",
      "966,1\n",
      "967,0\n",
      "968,0\n",
      "969,1\n",
      "970,0\n",
      "971,0\n",
      "972,1\n",
      "973,0\n",
      "974,0\n",
      "975,0\n",
      "976,0\n",
      "977,0\n",
      "978,0\n",
      "979,1\n",
      "980,0\n",
      "981,1\n",
      "982,0\n",
      "983,0\n",
      "984,1\n",
      "985,0\n",
      "986,0\n",
      "987,0\n",
      "988,1\n",
      "989,0\n",
      "990,1\n",
      "991,0\n",
      "992,1\n",
      "993,0\n",
      "994,0\n",
      "995,0\n",
      "996,1\n",
      "997,0\n",
      "998,0\n",
      "999,0\n",
      "1000,0\n",
      "1001,0\n",
      "1002,0\n",
      "1003,0\n",
      "1004,1\n",
      "1005,1\n",
      "1006,1\n",
      "1007,0\n",
      "1008,0\n",
      "1009,1\n",
      "1010,0\n",
      "1011,1\n",
      "1012,1\n",
      "1013,0\n",
      "1014,1\n",
      "1015,0\n",
      "1016,0\n",
      "1017,1\n",
      "1018,0\n",
      "1019,1\n",
      "1020,0\n",
      "1021,0\n",
      "1022,0\n",
      "1023,0\n",
      "1024,0\n",
      "1025,0\n",
      "1026,0\n",
      "1027,0\n",
      "1028,0\n",
      "1029,0\n",
      "1030,0\n",
      "1031,0\n",
      "1032,0\n",
      "1033,1\n",
      "1034,0\n",
      "1035,0\n",
      "1036,0\n",
      "1037,0\n",
      "1038,0\n",
      "1039,0\n",
      "1040,0\n",
      "1041,0\n",
      "1042,1\n",
      "1043,0\n",
      "1044,0\n",
      "1045,1\n",
      "1046,0\n",
      "1047,0\n",
      "1048,1\n",
      "1049,0\n",
      "1050,0\n",
      "1051,1\n",
      "1052,0\n",
      "1053,1\n",
      "1054,1\n",
      "1055,0\n",
      "1056,0\n",
      "1057,1\n",
      "1058,0\n",
      "1059,0\n",
      "1060,1\n",
      "1061,1\n",
      "1062,0\n",
      "1063,0\n",
      "1064,0\n",
      "1065,0\n",
      "1066,0\n",
      "1067,1\n",
      "1068,1\n",
      "1069,1\n",
      "1070,1\n",
      "1071,1\n",
      "1072,0\n",
      "1073,0\n",
      "1074,1\n",
      "1075,0\n",
      "1076,1\n",
      "1077,0\n",
      "1078,1\n",
      "1079,0\n",
      "1080,0\n",
      "1081,0\n",
      "1082,0\n",
      "1083,0\n",
      "1084,0\n",
      "1085,0\n",
      "1086,1\n",
      "1087,0\n",
      "1088,1\n",
      "1089,1\n",
      "1090,0\n",
      "1091,0\n",
      "1092,1\n",
      "1093,1\n",
      "1094,0\n",
      "1095,1\n",
      "1096,0\n",
      "1097,0\n",
      "1098,0\n",
      "1099,0\n",
      "1100,1\n",
      "1101,0\n",
      "1102,0\n",
      "1103,0\n",
      "1104,0\n",
      "1105,1\n",
      "1106,1\n",
      "1107,0\n",
      "1108,0\n",
      "1109,0\n",
      "1110,1\n",
      "1111,0\n",
      "1112,1\n",
      "1113,0\n",
      "1114,1\n",
      "1115,0\n",
      "1116,1\n",
      "1117,0\n",
      "1118,0\n",
      "1119,0\n",
      "1120,0\n",
      "1121,0\n",
      "1122,0\n",
      "1123,1\n",
      "1124,0\n",
      "1125,0\n",
      "1126,1\n",
      "1127,0\n",
      "1128,1\n",
      "1129,1\n",
      "1130,1\n",
      "1131,1\n",
      "1132,1\n",
      "1133,1\n",
      "1134,0\n",
      "1135,0\n",
      "1136,0\n",
      "1137,0\n",
      "1138,1\n",
      "1139,0\n",
      "1140,1\n",
      "1141,0\n",
      "1142,1\n",
      "1143,0\n",
      "1144,1\n",
      "1145,0\n",
      "1146,0\n",
      "1147,0\n",
      "1148,0\n",
      "1149,0\n",
      "1150,1\n",
      "1151,0\n",
      "1152,0\n",
      "1153,0\n",
      "1154,1\n",
      "1155,1\n",
      "1156,0\n",
      "1157,0\n",
      "1158,0\n",
      "1159,0\n",
      "1160,0\n",
      "1161,0\n",
      "1162,0\n",
      "1163,0\n",
      "1164,1\n",
      "1165,1\n",
      "1166,0\n",
      "1167,1\n",
      "1168,0\n",
      "1169,0\n",
      "1170,0\n",
      "1171,0\n",
      "1172,0\n",
      "1173,1\n",
      "1174,0\n",
      "1175,0\n",
      "1176,1\n",
      "1177,0\n",
      "1178,0\n",
      "1179,0\n",
      "1180,0\n",
      "1181,0\n",
      "1182,0\n",
      "1183,0\n",
      "1184,0\n",
      "1185,0\n",
      "1186,0\n",
      "1187,0\n",
      "1188,1\n",
      "1189,0\n",
      "1190,1\n",
      "1191,0\n",
      "1192,0\n",
      "1193,0\n",
      "1194,0\n",
      "1195,0\n",
      "1196,0\n",
      "1197,1\n",
      "1198,1\n",
      "1199,1\n",
      "1200,0\n",
      "1201,0\n",
      "1202,0\n",
      "1203,0\n",
      "1204,0\n",
      "1205,0\n",
      "1206,1\n",
      "1207,1\n",
      "1208,0\n",
      "1209,0\n",
      "1210,0\n",
      "1211,0\n",
      "1212,0\n",
      "1213,0\n",
      "1214,0\n",
      "1215,0\n",
      "1216,1\n",
      "1217,0\n",
      "1218,1\n",
      "1219,0\n",
      "1220,0\n",
      "1221,0\n",
      "1222,1\n",
      "1223,0\n",
      "1224,0\n",
      "1225,0\n",
      "1226,0\n",
      "1227,0\n",
      "1228,0\n",
      "1229,0\n",
      "1230,0\n",
      "1231,1\n",
      "1232,0\n",
      "1233,0\n",
      "1234,0\n",
      "1235,1\n",
      "1236,0\n",
      "1237,1\n",
      "1238,0\n",
      "1239,1\n",
      "1240,0\n",
      "1241,1\n",
      "1242,1\n",
      "1243,0\n",
      "1244,0\n",
      "1245,0\n",
      "1246,1\n",
      "1247,0\n",
      "1248,1\n",
      "1249,0\n",
      "1250,0\n",
      "1251,1\n",
      "1252,0\n",
      "1253,1\n",
      "1254,1\n",
      "1255,0\n",
      "1256,1\n",
      "1257,0\n",
      "1258,0\n",
      "1259,0\n",
      "1260,1\n",
      "1261,0\n",
      "1262,0\n",
      "1263,1\n",
      "1264,0\n",
      "1265,0\n",
      "1266,1\n",
      "1267,1\n",
      "1268,0\n",
      "1269,0\n",
      "1270,0\n",
      "1271,0\n",
      "1272,0\n",
      "1273,0\n",
      "1274,0\n",
      "1275,0\n",
      "1276,0\n",
      "1277,1\n",
      "1278,0\n",
      "1279,0\n",
      "1280,0\n",
      "1281,0\n",
      "1282,0\n",
      "1283,1\n",
      "1284,0\n",
      "1285,0\n",
      "1286,0\n",
      "1287,1\n",
      "1288,0\n",
      "1289,1\n",
      "1290,0\n",
      "1291,0\n",
      "1292,1\n",
      "1293,0\n",
      "1294,1\n",
      "1295,1\n",
      "1296,0\n",
      "1297,0\n",
      "1298,0\n",
      "1299,0\n",
      "1300,0\n",
      "1301,1\n",
      "1302,0\n",
      "1303,1\n",
      "1304,1\n",
      "1305,0\n",
      "1306,1\n",
      "1307,0\n",
      "1308,0\n",
      "1309,1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "pd.set_option('display.max_columns', 15)\n",
    "\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "mask4 = (df.Age < 10) | (df.Sex == 'female')\n",
    "df['child_women'] = mask4.astype(int)\n",
    "\n",
    "# most_freq = rdf['Age'].value_counts(dropna=True).idxmax()\n",
    "# rdf['Age'].fillna(most_freq, inplace=True)\n",
    "\n",
    "appel = df['Name'].str.split(',').str[1].str.split('.')\n",
    "df['title'] = appel.str.get(0)\n",
    "age_m = pd.DataFrame({'mean_age':round(df.groupby('title')['Age'].mean(),1)})\n",
    "age_m\n",
    "\n",
    "df = pd.merge(df, age_m, on='title',how='left')\n",
    "df.Age.fillna(df.mean_age, inplace=True)\n",
    "\n",
    "most_freq = df['Embarked'].value_counts().idxmax()\n",
    "df['Embarked'].fillna(most_freq, inplace=True)\n",
    "\n",
    "local_std = df.Fare.std() * 5\n",
    "df = df[:][df['Fare'] < local_std]\n",
    "\n",
    "rdf = df.drop(['PassengerId', 'Cabin', 'Name', 'Ticket', 'mean_age','title'], axis=1)\n",
    "\n",
    "ndf = rdf\n",
    "\n",
    "gender = pd.get_dummies(ndf['Sex'])\n",
    "ndf = pd.concat([ndf, gender], axis=1)\n",
    "onehot_embarked = pd.get_dummies(ndf['Embarked'])\n",
    "ndf = pd.concat([ndf, onehot_embarked], axis=1)\n",
    "ndf.drop(['Sex', 'Embarked'], axis=1, inplace=True)\n",
    "\n",
    "x = ndf[['Fare', 'Pclass', 'Age', 'SibSp', 'Parch', 'female', 'male', 'C', 'Q', 'S', 'child_women']]\n",
    "y = ndf['Survived']  # 종속변수\n",
    "\n",
    "X = preprocessing.StandardScaler().fit(x).transform(x)\n",
    "\n",
    "\"\"\"\n",
    "test data\n",
    "\"\"\"\n",
    "\n",
    "x_test = pd.read_csv(\"test.csv\")\n",
    "mask4 = (x_ktest.Age < 10) | (x_ktest.Sex == 'female')\n",
    "x_test['child_women'] = mask4.astype(int)\n",
    "\n",
    "# most_freq = rdf_x_ktest['Age'].value_counts(dropna=True).idxmax()\n",
    "# rdf_x_ktest['Age'].fillna(most_freq, inplace=True)\n",
    "\n",
    "appel = x_test['Name'].str.split(',').str[1].str.strip().str.split('.')\n",
    "x_test['title'] = appel.str.get(0)\n",
    "\n",
    "test_age_m = pd.DataFrame({'mean_age':round(x_test.groupby('title')['Age'].mean(),1)})\n",
    "test_age_m.loc['Ms']=28.0\n",
    "\n",
    "x_test = pd.merge(x_test, test_age_m, on='title',how='left')\n",
    "x_test.Age.fillna(x_test.mean_age, inplace=True)\n",
    "\n",
    "most_freq = x_test['Embarked'].value_counts().idxmax()\n",
    "x_test['Embarked'].fillna(most_freq, inplace=True)\n",
    "\n",
    "most_freq = x_test['Fare'].value_counts().idxmax()\n",
    "x_test['Fare'].fillna(most_freq, inplace=True)\n",
    "\n",
    "rdf_x_ktest = x_test.drop(['PassengerId', 'Cabin', 'Name', 'Ticket','mean_age','title'], axis=1,)\n",
    "ndf_x_ktest = rdf_x_ktest\n",
    "\n",
    "\n",
    "gender = pd.get_dummies(ndf_x_ktest['Sex'])\n",
    "ndf_x_ktest = pd.concat([ndf_x_ktest, gender], axis=1)\n",
    "onehot_embarked = pd.get_dummies(ndf_x_ktest['Embarked'])\n",
    "ndf_x_ktest = pd.concat([ndf_x_ktest, onehot_embarked], axis=1)\n",
    "ndf_x_ktest.drop(['Sex', 'Embarked'], axis=1, inplace=True)\n",
    "\n",
    "x = ndf_x_ktest[['Fare', 'Pclass', 'Age', 'SibSp', 'Parch','female', 'male', 'C', 'Q', 'S', 'child_women']]\n",
    "\n",
    "X_test = preprocessing.StandardScaler().fit(x).transform(x)\n",
    "\n",
    "tree_model = RandomForestClassifier(n_estimators=400, oob_score=False,).fit(X, y)\n",
    "\n",
    "y_hat = tree_model.predict(X_test)\n",
    "\n",
    "for i, a in enumerate(y_hat):\n",
    "    print(str(i+892) + ',' + str(a))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
